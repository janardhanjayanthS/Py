from fastapi import APIRouter, HTTPException, status
from langchain_core.messages import HumanMessage

from src.core.ai_utility import (
    calculate_token_cost,
    clean_llm_output,
    get_agent,
)
from src.core.constants import MESSAGES, AIModels, ResponseType, logger
from src.schema.ai import Query
from src.schema.response import APIResponse

ai = APIRouter()


@ai.post("/ai/query", response_model=APIResponse)
async def query_response(query: Query):
    """
    Handles a direct query to an LLM agent without RAG (Retrieval-Augmented Generation).

    The user's query is added to the session messages, and the response is
    generated by a general-purpose AI model (default: GPT-4o_MINI).
    Calculates and returns the token cost for the interaction.

    Args:
        query: The user's query wrapped in a Query schema object.

    Returns:
        A dictionary with the response status and a message containing the
        AI's reply and the calculated token cost.

    Raises:
        Exception: Catches and reports any general error during the agent invocation.
    """
    MESSAGES.append(HumanMessage(content=query.query))

    ai_model: AIModels = AIModels.GPT_4o_MINI

    try:
        agent = get_agent(ai_model=ai_model)
        agent_response = await agent.ainvoke(MESSAGES)

        ai_reply = agent_response.content
        token_cost = calculate_token_cost(
            agent_response.usage_metadata, ai_model=ai_model
        )

        return APIResponse(
            response_type=ResponseType.SUCCESS,
            message={
                "ai response": clean_llm_output(ai_reply),
                "token cost": token_cost,
            },
        )
    except Exception as e:
        logger.error(f"Error {e}")
        raise HTTPException(
            status=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="Error while processing AI query. Please try again.",
        )
