from fastapi import APIRouter, File, HTTPException, UploadFile, status
from langchain_core.messages import HumanMessage
from src.core.ai_utility import (
    calculate_token_cost,
    clean_llm_output,
    get_agent,
    get_conversational_rag_chain,
    update_history,
)
from src.core.constants import HISTORY, MESSAGES, AIModels, ResponseType, logger
from src.core.database import (
    add_file_as_embedding,
    add_web_content_as_embedding,
)
from src.schema.ai import BlogLink, Query

ai = APIRouter()


@ai.post("/ai/query")
async def query_response(query: Query):
    """
    Handles a direct query to an LLM agent without RAG (Retrieval-Augmented Generation).

    The user's query is added to the session messages, and the response is
    generated by a general-purpose AI model (default: GPT-4o_MINI).
    Calculates and returns the token cost for the interaction.

    Args:
        query: The user's query wrapped in a Query schema object.

    Returns:
        A dictionary with the response status and a message containing the
        AI's reply and the calculated token cost.

    Raises:
        Exception: Catches and reports any general error during the agent invocation.
    """
    MESSAGES.append(HumanMessage(content=query.query))

    ai_model: AIModels = AIModels.GPT_4o_MINI

    try:
        agent = get_agent(ai_model=ai_model)
        agent_response = agent.invoke(MESSAGES)

        ai_reply = agent_response.content
        token_cost = calculate_token_cost(
            agent_response.usage_metadata, ai_model=ai_model
        )

        return {
            "response": ResponseType.SUCCESS.value,
            "message": {"ai response": ai_reply, "token cost": token_cost},
        }
    except Exception as e:
        return {
            "response": ResponseType.ERROR.value,
            "message": f"{e}",
        }


@ai.post("/ai/db_query")
async def search_from_db(query: Query):
    """
    Performs a query using a Conversational RAG chain against the vector database.

    The query is answered based on the documents stored in the database,
    maintaining conversational history. The chat history is updated after
    the response is generated. Calculates and returns the token cost.

    Args:
        query: The user's query wrapped in a Query schema object.

    Returns:
        A dictionary with the response status and a message containing the
        cleaned AI's response and the calculated token cost.

    Raises:
        HTTPException: If an error occurs during the RAG chain invocation,
                       a 400 Bad Request is raised.
    """
    try:
        result = get_conversational_rag_chain().invoke(
            {"question": query.query, "chat_history": HISTORY}
        )
        update_history(result=result, query=query)
        token_cost = calculate_token_cost(
            result.usage_metadata, ai_model=AIModels.GPT_4o_MINI
        )

        return {
            "response": ResponseType.SUCCESS.value,
            "message": {
                "token cost": token_cost,
                "query response": clean_llm_output(result.content),
            },
        }
    except Exception as e:
        message = f"Error {e}"
        logger.error(message)
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=message)


@ai.post("/ai/upload_file")
async def upload_pdf_to_db(file: UploadFile = File(...)):
    """
    Uploads a PDF file and stores its content as embeddings in the vector database.

    Only accepts files with a '.pdf' extension. Reads the file content and
    calls the function to process and add the embeddings.

    Args:
        file: The uploaded file, expected to be a PDF.

    Returns:
        A dictionary with the response status and a message containing the
        database's response upon successful embedding addition.

    Raises:
        HTTPException: If an error occurs during file reading or embedding
                       generation, a 400 Bad Request is raised.
    """
    if not file.filename.endswith(".pdf"):
        message = "Only supports pdf files"
        logger.error(message)
        return {
            "response": ResponseType.ERROR.value,
            "message": message,
        }

    try:
        contents = await file.read()
        file_add_response = add_file_as_embedding(
            contents=contents, filename=file.filename
        )
        return {
            "response": ResponseType.SUCCESS.value,
            "message": {
                "db response": file_add_response,
            },
        }
    except Exception as e:
        message = f"Error {e}"
        logger.error(message)
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=message)


@ai.post("/ai/upload_blog")
async def upload_blog_to_db(blog_url: BlogLink):
    """
    Fetches content from a specified URL (e.g., a blog) and stores it as
    embeddings in the vector database.

    The URL is extracted from the BlogLink schema and the web content is
    processed for embedding generation.

    Args:
        blog_url: A BlogLink schema object containing the URL of the web content.

    Returns:
        A dictionary with the response status and a message containing the
        database's response upon successful embedding addition.

    Raises:
        HTTPException: If an error occurs during web content retrieval or
                       embedding generation, a 400 Bad Request is raised.
    """
    try:
        web_upload_result = add_web_content_as_embedding(url=str(blog_url.url))
        return {
            "response": ResponseType.SUCCESS.value,
            "message": {
                "db response": web_upload_result,
            },
        }
    except Exception as e:
        message = f"Error {e}"
        logger.error(message)
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail=message)
