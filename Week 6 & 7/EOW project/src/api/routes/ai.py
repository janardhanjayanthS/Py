from fastapi import APIRouter
from langchain_core.messages import HumanMessage
from src.core.ai_utility import (
    calculate_token_cost,
    clean_llm_output,
    get_agent,
)
from src.core.constants import MESSAGES, AIModels, ResponseType
from src.schema.ai import Query

ai = APIRouter()


@ai.post("/ai/query")
async def query_response(query: Query):
    """
    Handles a direct query to an LLM agent without RAG (Retrieval-Augmented Generation).

    The user's query is added to the session messages, and the response is
    generated by a general-purpose AI model (default: GPT-4o_MINI).
    Calculates and returns the token cost for the interaction.

    Args:
        query: The user's query wrapped in a Query schema object.

    Returns:
        A dictionary with the response status and a message containing the
        AI's reply and the calculated token cost.

    Raises:
        Exception: Catches and reports any general error during the agent invocation.
    """
    MESSAGES.append(HumanMessage(content=query.query))

    ai_model: AIModels = AIModels.GPT_4o_MINI

    try:
        agent = get_agent(ai_model=ai_model)
        agent_response = agent.invoke(MESSAGES)

        ai_reply = agent_response.content
        token_cost = calculate_token_cost(
            agent_response.usage_metadata, ai_model=ai_model
        )

        return {
            "response": ResponseType.SUCCESS.value,
            "message": {
                "ai response": clean_llm_output(ai_reply),
                "token cost": token_cost,
            },
        }
    except Exception as e:
        return {
            "response": ResponseType.ERROR.value,
            "message": f"{e}",
        }
