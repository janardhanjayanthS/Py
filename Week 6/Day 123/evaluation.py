from json import JSONDecodeError, load
from typing import Optional

from constants import GPT_4o_MINI, client
from langchain_core.prompts import PromptTemplate
from prompt import SYSTEM_PROMPT_EVALUATION, USER_MESSAGE_EVALUATION


def evaluate_llm_response(llm_response: str, user_query: str) -> str:
    """Evaluates an LLM-generated response against ideal Q&A examples.

    Constructs evaluation messages using system and user prompts, then calls
    the evaluation model to assess the quality of the LLM response.

    Args:
        llm_response: The response generated by the LLM to be evaluated.
        user_query: The original user question/query that prompted the response.

    Returns:
        str: The evaluation result from the evaluation model, typically containing
            a score and reasoning for the assessment.

    Raises:
        ValueError: If Q&A dataset cannot be loaded.
        Exception: If the evaluation model API call fails.
    """
    messages = [
        {"role": "system", "content": SYSTEM_PROMPT_EVALUATION},
        {
            "role": "user",
            "content": get_user_message_for_evaluation_model(
                user_query=user_query, llm_response=llm_response
            ),
        },
    ]
    return call_evaluation_model(messages=messages)


def get_user_message_for_evaluation_model(user_query: str, llm_response: str) -> str:
    """Formats the user message for the evaluation model with ideal examples.

    Loads ideal Q&A pairs from a JSON file and formats them along with the
    user query and LLM response into the evaluation prompt template.

    Args:
        user_query: The original user question/query.
        llm_response: The LLM-generated response to be evaluated.

    Returns:
        str: A formatted evaluation prompt containing the user query, ideal Q&A
            examples, and the LLM response to be evaluated.

    Raises:
        ValueError: If Q&A dataset cannot be loaded from file.
    """
    qa_data = get_qa_data(filename="qa_dataset.json")
    if qa_data is None:
        raise ValueError("Failed to load Q&A dataset. Cannot proceed with evaluation.")

    eval_prompt_template = PromptTemplate(
        input_variables=["user_message", "ideal", "llm_response"],
        template=USER_MESSAGE_EVALUATION,
    )

    formatted_user_message_for_evaluation_model = eval_prompt_template.format(
        user_message=user_query, ideal=qa_data, llm_response=llm_response
    )
    return formatted_user_message_for_evaluation_model


def call_evaluation_model(messages: list) -> str:
    """Calls the evaluation model API with the provided messages.

    Args:
        messages: A list of message dictionaries containing role and content
            keys, formatted for the chat completion API.

    Returns:
        str: The text content of the evaluation model's response.

    Raises:
        Exception: If the API call fails due to network issues, rate limits,
            or other API errors.
    """
    try:
        response = client.chat.completions.create(model=GPT_4o_MINI, messages=messages)
        return response.choices[0].message.content
    except Exception as e:
        print(f"Error calling evaluation model: {e}")
        raise


def get_qa_data(filename: str) -> Optional[list]:
    """Loads Q&A dataset from a JSON file.

    Args:
        filename: Path to the JSON file containing the Q&A dataset.

    Returns:
        Optional[list]: The loaded Q&A data as a list if successful, None if
            the file is not found or cannot be loaded.

    Note:
        Prints an error message to stdout if file cannot be loaded.
    """
    try:
        with open(filename, "r") as json_file:
            data = load(json_file)
        return data
    except FileNotFoundError as e:
        print(f"Error: File not found - {e}")
        return None
    except JSONDecodeError as e:
        print(f"Error: Invalid JSON format - {e}")
        return None
    except Exception as e:
        print(f"Error loading Q&A data: {e}")
        return None
